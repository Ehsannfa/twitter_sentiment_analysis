{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab57a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " len(self.stopwords) =  0\n",
      "\n",
      " train size ==  31962 Index(['id', 'label', 'tweet'], dtype='object')\n",
      "\n",
      " test size ==  17197\n",
      "\n",
      " summary : \n",
      "    id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ba7416c13f74>:120: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['tidy_tweet'] == train['tidy_tweet'].str.replace(\"^A-Za-z#\", \" \")\n",
      "<ipython-input-9-ba7416c13f74>:129: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['tidy_tweet'] == test['tidy_tweet'].str.replace(\"^A-Za-z#\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " embedding_matrix =  (300000, 300)\n",
      "Epoch 1/2\n",
      "44/44 [==============================] - 169s 4s/step - loss: 0.2358 - accuracy: 0.9306\n",
      "Epoch 2/2\n",
      "44/44 [==============================] - 161s 4s/step - loss: 0.1366 - accuracy: 0.9449\n",
      "\n",
      " evaluation accuracy ===  0.17488501965999603\n",
      "\n",
      " len(test_labels) ===  17197\n",
      "\n",
      " id len ===  17197\n",
      "\n",
      " label ===  17197\n",
      "\n",
      " saved results in csv successfully === \n"
     ]
    }
   ],
   "source": [
    "import re, os, traceback\n",
    "from unicodedata import normalize\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from many_stop_words import get_stop_words\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "class TwitterSentimentAnalysis(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.embedding_dim = 300\n",
    "        \n",
    "        # configurable parameters\n",
    "        self.max_review_length = 500\n",
    "        self.top_words = 300000\n",
    "        self.test_size = 0.3\n",
    "        self.lemma_flag = 1\n",
    "        self.max_letters = 2\n",
    "        self.threshold = 0.5\n",
    "        self.remove_stopwords = 1\n",
    "        self.batch_size = 512\n",
    "        self.epochs = 2\n",
    "\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        if self.remove_stopwords: \n",
    "            # self.stopWords = list(get_stop_words('en'))\n",
    "            self.stopwords = []\n",
    "            print(\"\\n len(self.stopwords) = \", len(self.stopwords))\n",
    "        self.tokenizer = Tokenizer(num_words=self.top_words)\n",
    "\n",
    "    \n",
    "    def readData(self):\n",
    "        # step 1 === read dataset\n",
    "        train  = pd.read_csv('train_E6oV3lV.csv')\n",
    "        test = pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "        \n",
    "        print('\\n train size == ', len(train), train.keys())\n",
    "        print('\\n test size == ', len(test))\n",
    "        print(\"\\n summary : \\n\",train.head())\n",
    "        return train, test\n",
    "        \n",
    "\n",
    "    def clean_str(self, string): # unused\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        return string.strip().lower()\n",
    "\n",
    "\n",
    "    def removeStopWords(self, text):\n",
    "        return \" \".join([token for token in text.split() if token not in self.stopwords])\n",
    "            \n",
    "\n",
    "    \n",
    "    def checkLemma(self, wrd):\n",
    "        return nltk.stem.WordNetLemmatizer().lemmatize(nltk.stem.WordNetLemmatizer().lemmatize(wrd, 'v'), 'n')\n",
    "\n",
    "    \n",
    "    def getLemma(self, text):\n",
    "        text_list = []\n",
    "        text_list = [self.checkLemma(tok) for tok in text.lower().split()]\n",
    "        text = \" \".join(text_list)\n",
    "        return text\n",
    "\n",
    "    \n",
    "    ## function to remove twitter handle\n",
    "    def remove_pattern(self, txt, pattern):\n",
    "        r = re.findall(pattern, txt)\n",
    "        for i in r:\n",
    "            txt = re.sub(i, '', txt)\n",
    "            txt = re.sub(r\"[^A-Za-z0-9]\", \" \", txt)\n",
    "            # txt = txt.decode('utf-8')\n",
    "            # txt = normalize('NFKD', txt).encode('ASCII', 'ignore')\n",
    "            txt = \" \".join([str(word) for word in txt.split() if word not in string.ascii_letters])\n",
    "        return txt\n",
    "\n",
    "\n",
    "    def dataPreprocessing(self, train, test):\n",
    "        ############################################## train\n",
    "        train['tidy_tweet'] = np.vectorize(self.remove_pattern)(train['tweet'], \"@[\\w]*\")\n",
    "        train['tidy_tweet'] == train['tidy_tweet'].str.replace(\"^A-Za-z#\", \" \")\n",
    "        train['tidy_tweet'] = train['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>self.max_letters and w not in self.stopwords]))\n",
    "        train['tidy_tweet'] = [ self.clean_str(sent) for sent in train['tidy_tweet']]\n",
    "\n",
    "        if self.lemma_flag: train['tidy_tweet'] = [ self.getLemma(sent) for sent in train['tidy_tweet']]\n",
    "        tokenized_tweet_train = train['tidy_tweet'].apply(lambda x : x.split())   \n",
    "        \n",
    "        ############################################## test\n",
    "        test['tidy_tweet'] = np.vectorize(self.remove_pattern)(test['tweet'], \"@[\\w]*\")\n",
    "        test['tidy_tweet'] == test['tidy_tweet'].str.replace(\"^A-Za-z#\", \" \")\n",
    "        test['tidy_tweet'] = test['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>self.max_letters and w not in self.stopwords]))\n",
    "        test['tidy_tweet'] = [ self.clean_str(sent) for sent in test['tidy_tweet']]\n",
    "\n",
    "        if self.lemma_flag: test['tidy_tweet'] = [ self.getLemma(sent) for sent in test['tidy_tweet']]\n",
    "        tokenized_tweet_test = test['tidy_tweet'].apply(lambda x : x.split())   \n",
    "        \n",
    "        return tokenized_tweet_train, tokenized_tweet_test\n",
    "\n",
    "\n",
    "    # ste 5 === load word2vec\n",
    "    def loadw2vLocal(self):\n",
    "        if not os.path.exists('GoogleNews-vectors-negative300.bin'):\n",
    "            raise ValueError('google word2vec model is not there !! ')\n",
    "        \n",
    "        model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit=600000, binary=True)\n",
    "        return model\n",
    "\n",
    "    def createEmbeddingLayer(self, word_index, w2vmodel):\n",
    "        # ste 6 === create embedding matrix\n",
    "        embedding_matrix = np.zeros((self.top_words, self.embedding_dim))\n",
    "        \n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.top_words:\n",
    "                continue\n",
    "            else:\n",
    "                embedding_vector = np.zeros((1, self.embedding_dim)) # vector of 1 x 300\n",
    "                try:\n",
    "                    embedding_vector = w2vmodel[word]\n",
    "                except:\n",
    "                    pass\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        print(\"\\n embedding_matrix = \", embedding_matrix.shape)\n",
    "        embed_layer = Embedding(self.top_words, self.embedding_dim, weights=[embedding_matrix], input_length=self.max_review_length)\n",
    "        return embed_layer\n",
    "\n",
    "    def train(self, embed_layer, x_tr, y_tr, x_val, y_val):\n",
    "        #step 7 ===  create the model\n",
    "        model = Sequential()\n",
    "        model.add(embed_layer)\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(200)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='relu'))\n",
    "        \n",
    "        model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model.fit(x_tr, y_tr, epochs=self.epochs, batch_size=self.batch_size)\n",
    "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "        \n",
    "        print('\\n evaluation accuracy === ',scores[0])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def test(self,tokenized_tweet_test, model):\n",
    "        sequences = self.tokenizer.texts_to_sequences(tokenized_tweet_test)\n",
    "        x_predict = sequence.pad_sequences(sequences, maxlen=self.max_review_length)\n",
    "        y_prob = model.predict(x_predict)\n",
    "        \n",
    "        test_results = [lst[0] for lst in y_prob]\n",
    "        test_labels = [0 if score < self.threshold else 1 for score in test_results ]\n",
    "        \n",
    "        print(\"\\n len(test_labels) === \",len(test_labels))\n",
    "        return test_labels\n",
    "\n",
    "\n",
    "    def writeToCsv(self, test, test_labels):\n",
    "        try:\n",
    "            submission = defaultdict(list)\n",
    "            submission['id'].extend(test['id'])\n",
    "            submission['label'].extend(test_labels)\n",
    "        \n",
    "            print(\"\\n id len === \",len(submission['id']))\n",
    "            print(\"\\n label === \", len(submission['label']))\n",
    "        \n",
    "            submission = pd.DataFrame(submission)\n",
    "            writer = ExcelWriter('submission_200.xlsx')\n",
    "            submission.to_excel(writer)\n",
    "            writer.save()\n",
    "            print(\"\\n saved results in csv successfully\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"\\n error \", e, \"\\n traceback\",traceback.format_exc())\n",
    "    \n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "        train, test = self.readData()\n",
    "        tokenized_tweet_train, tokenized_tweet_test = self.dataPreprocessing(train, test)\n",
    "        x_tr, x_val, y_tr, y_val = train_test_split( tokenized_tweet_train, train['label'], test_size = self.test_size, random_state=42 )\n",
    "\n",
    "        self.tokenizer.fit_on_texts(x_tr)\n",
    "        sequences = self.tokenizer.texts_to_sequences(x_tr)\n",
    "        word_index = self.tokenizer.word_index\n",
    "        x_tr = sequence.pad_sequences(sequences, maxlen=self.max_review_length)\n",
    "        \n",
    "        sequences = self.tokenizer.texts_to_sequences(x_val)\n",
    "        x_val = sequence.pad_sequences(sequences, maxlen=self.max_review_length)\n",
    "        \n",
    "        w2vmodel = self.loadw2vLocal()\n",
    "\n",
    "        embed_layer = self.createEmbeddingLayer(word_index, w2vmodel)\n",
    "        model = self.train(embed_layer, x_tr, y_tr, x_val, y_val)\n",
    "        test_labels = self.test(tokenized_tweet_test, model)\n",
    "        self.writeToCsv(test, test_labels)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    obj = TwitterSentimentAnalysis()\n",
    "    obj.main()\n",
    "    \n",
    "### improvements\n",
    "# 1. stopwords\n",
    "# 2. lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57476184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: many-stop-words in /opt/anaconda3/lib/python3.8/site-packages (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c6f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
